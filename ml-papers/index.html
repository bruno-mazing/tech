<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
	<meta name="author" content="Bruno Mazing">
  <title>Bruno Mazing - machine learning</title>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Neuton:400,400i,700,700i' rel='stylesheet'>
</head>

<body>
<header>
  <h1><a href="./">Bruno Mazing</a></h1></header>

  <h3>Paper presentation candidates</h3>
  <ul>
    <li>An image is worth 16x16 words: Transformers for image recognition at scale (<a href="https://arxiv.org/pdf/2010.11929.pdf">arxiv</a>).</li>
    <li>Inductive biases and variable creation in self-attention mechanisms (<a href="https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf">link</a>).</li>
  <li>A Wigner-Eckart theorem for group-equivariant convolution kernels (<a href="https://arxiv.org/abs/2010.10952">arxiv</a>).</li>
  <li>Equivariant architectures for learning in deep weight spaces (<a href="https://arxiv.org/pdf/2301.12780.pdf">arxiv</a>).</li>
  <li>Scalars are universal: equivariant machine learning, structured like classical physics (<a href="https://arxiv.org/pdf/2106.06610.pdf">arxiv</a>).</li>
<li>Implicit bias of linear equivariant networks (<a href="https://arxiv.org/abs/2110.06084">arxiv</a>).</li>
<li>Provably strict generalization benefit for equivariant models (<a href="https://arxiv.org/abs/2102.10333">arxiv</a>).</li>
<li>On the universality of rotation equivariant point cloud networks (<a href="https://arxiv.org/pdf/2010.02449.pdf">arxiv</a>).</li>
<li>Equivariant flows: exact likelihood generative learning for symmetric densities (<a href="https://arxiv.org/pdf/2006.02425.pdf">arxiv</a>).</li>
<li>Sign and basis invariant networks for spectral graph representation learning (<a href="https://openreview.net/pdf?id=Q-UHqMorzil">link</a>).</li>

  </ul>




  <h3>Reading Lists</h3>
  <ul>
    <li><a href="https://ml.berkeley.edu/reading-list/">UC Berkeley ML List</a>. Possibly relevant papers include (1) Autoaugment, (2) RotNet, (3) MAML. See also the link to the MAML tutorial at the bottom of the page.</li>
    <li>Machine Learning Research Papers <a href="https://github.com/anubhavshrimal/Machine-Learning-Research-Papers">github</a></li>
    <li>UBC reading group <a href="https://ml.ubc.ca/mlrg/">link</a>.</li>
    <li>Rosanne Liu's list <a href="https://rosanneliu.com/dlct/">link</a>.</li>
    <li></li>
    <li><a href="https://www.reddit.com/r/MachineLearning/comments/cghf6r/d_nononsense_comprehensive_reading_list_for_ml_ds/">from Reddit</a></li>
    <li><a href="https://aman.ai/read/">Amain's AI list</a></li>
    <li><a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap">More stuff</a></li>

    <li><a href="https://towardsdatascience.com/7-best-research-papers-to-read-to-get-started-with-deep-learning-projects-59e11f7b9c32">Need a Medium account for this one</a>.</li>
  </ul>

  <h3>Topics</h3>
  Some machine learning research papers and topics of interest:

  <ul>
    <li>Zoom In: a gentle intro to circuits <a href="https://distill.pub/2020/circuits/zoom-in/">link</a>.</li>
    <li>Diffusion</li>
    <li>Neural ODEs</li>
    <li>Normalizing Flows: An Introduction and Review of Current Methods (<a href="https://arxiv.org/abs/1908.09257">arxiv</a>). See also "Flow-based generative model" on wikipedia (<a href="https://en.wikipedia.org/wiki/Flow-based_generative_model">link</a>).</li>
    <li>The Kernel Method, wikipedia (<a href="https://en.wikipedia.org/wiki/Kernel_method">link</a>).</li>


  </ul>


  <h3>Books</h3>
  <ul>
    <li>PyTorch book</li>
    <li>Deep Learning and the game of Go.</li>
  </ul>

<h3>Links</h3>
<ul>
  <li>Machine Learning Mastery <a href="https://machinelearningmastery.com/">website</a>.</li>
  <li>JAX: High-Performance Array Computing (<a href="https://jax.readthedocs.io/en/latest/index.html">link</a>).</li>
</ul>


  </body>
  </html>
